import streamlit as st
import numpy as np
import librosa
import pickle
from tensorflow.keras.models import load_model

# =========================
# ğŸ¨ Page Config
# =========================
st.set_page_config(page_title="Speech Emotion Recognition", page_icon="ğŸ¤", layout="centered")

# =========================
# ğŸ“Œ Load Model and Label Encoder
# =========================
@st.cache_resource
def load_model_and_encoder():
    model = load_model("best_emotion_model.keras")  # new improved model
    with open("label_encoder.pkl", "rb") as f:
        le = pickle.load(f)
    return model, le

model, le = load_model_and_encoder()
# =========================
# ğŸ” Show loaded classes for debugging
# =========================
st.sidebar.title("ğŸ” Debug Info")

# Show loaded classes
st.sidebar.write("Loaded Classes:", list(le.classes_))

# Show model output shape
st.sidebar.write("Model Output Shape:", model.output_shape)

# =========================
# ğŸ“Œ MFCC Feature Extraction
# =========================
def extract_features(file_path, max_pad_len=174, n_mfcc=40):
    audio, sample_rate = librosa.load(file_path, sr=None)
    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)
    
    # Pad or truncate
    if mfccs.shape[1] < max_pad_len:
        pad_width = max_pad_len - mfccs.shape[1]
        mfccs = np.pad(mfccs, pad_width=((0,0),(0,pad_width)), mode="constant")
    else:
        mfccs = mfccs[:, :max_pad_len]
    
    return mfccs[..., np.newaxis]  # shape -> (40, 174, 1)

# =========================
# ğŸ“Œ Prediction Function
# =========================
def predict_emotion(file_path):
    features = extract_features(file_path)
    features = np.expand_dims(features, axis=0)
    prediction = model.predict(features, verbose=0)
    predicted_idx = np.argmax(prediction, axis=1)[0]
    predicted_label = le.inverse_transform([predicted_idx])[0]
    return predicted_label, prediction[0]

# =========================
# ğŸ¤ Streamlit UI
# =========================
st.title("ğŸ¤ Speech Emotion Recognition App")
st.write("Upload a short `.wav` file (3â€“4 seconds) and let the model detect the emotion.")

uploaded_file = st.file_uploader("ğŸ“‚ Choose an audio file", type=["wav"])

if uploaded_file is not None:
    temp_path = "temp.wav"
    with open(temp_path, "wb") as f:
        f.write(uploaded_file.read())
    
    st.audio(temp_path, format="audio/wav")
    
    with st.spinner("ğŸ” Analyzing audio..."):
        emotion, probabilities = predict_emotion(temp_path)

    # Emojis for emotions
    emotion_icons = {
        "neutral": "ğŸ˜",
        "calm": "ğŸ˜Œ",
        "happy": "ğŸ˜„",
        "sad": "ğŸ˜¢",
        "angry": "ğŸ˜¡",
        "fearful": "ğŸ˜¨",
        "disgust": "ğŸ¤¢",
        "surprised": "ğŸ˜²"
    }
    icon = emotion_icons.get(emotion, "â“")
    
    # Show results
    st.success(f"### Predicted Emotion: {icon} {emotion.capitalize()}")
    
    # Show confidence
    st.subheader("Prediction Confidence")
    prob_dict = dict(zip(le.classes_, probabilities))
    st.bar_chart(prob_dict)
